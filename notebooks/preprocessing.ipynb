{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/turing/miniconda/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import pickle\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from scipy.signal import medfilt\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from tsforest.utils import make_time_range\n",
    "import holidays\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# local modules\n",
    "import sys\n",
    "sys.path.append(\"../lib/\")\n",
    "from utils import compute_scaling, reduce_mem_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30490 entries, 0 to 30489\n",
      "Columns: 1947 entries, id to d_1941\n",
      "dtypes: int64(1941), object(6)\n",
      "memory usage: 452.9+ MB\n"
     ]
    }
   ],
   "source": [
    "sales_train = pd.read_csv(\"../input/sales_train_evaluation.csv\")\n",
    "sales_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1969 entries, 0 to 1968\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   date          1969 non-null   datetime64[ns]\n",
      " 1   wm_yr_wk      1969 non-null   int64         \n",
      " 2   weekday       1969 non-null   object        \n",
      " 3   wday          1969 non-null   int64         \n",
      " 4   month         1969 non-null   int64         \n",
      " 5   year          1969 non-null   int64         \n",
      " 6   d             1969 non-null   object        \n",
      " 7   event_name_1  162 non-null    object        \n",
      " 8   event_type_1  162 non-null    object        \n",
      " 9   event_name_2  5 non-null      object        \n",
      " 10  event_type_2  5 non-null      object        \n",
      " 11  snap_CA       1969 non-null   int64         \n",
      " 12  snap_TX       1969 non-null   int64         \n",
      " 13  snap_WI       1969 non-null   int64         \n",
      "dtypes: datetime64[ns](1), int64(7), object(6)\n",
      "memory usage: 215.5+ KB\n"
     ]
    }
   ],
   "source": [
    "calendar = pd.read_csv(\"../input/calendar.csv\", parse_dates=[\"date\"])\n",
    "calendar.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6841121 entries, 0 to 6841120\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Dtype  \n",
      "---  ------      -----  \n",
      " 0   store_id    object \n",
      " 1   item_id     object \n",
      " 2   wm_yr_wk    int64  \n",
      " 3   sell_price  float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 208.8+ MB\n"
     ]
    }
   ],
   "source": [
    "sell_prices = pd.read_csv(\"../input/sell_prices.csv\")\n",
    "sell_prices.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Events features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_holidays = holidays.UnitedStates(years=[2011,2012,2013,2014,2015,2016], observed=True)\n",
    "holidays_dataframe = pd.DataFrame(us_holidays.items(), columns=[\"ds\",\"event\"])\n",
    "holidays_dataframe.sort_values(\"ds\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### long weekends features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw_features = make_time_range(\"2011-01-29\", \"2016-06-19\", \"D\")\n",
    "lw_features.set_index(\"ds\", inplace=True)\n",
    "lw_features[\"lw_type\"] = 0\n",
    "lw_features[\"lw_day\"] = 0\n",
    "lw_features[\"prev_lw\"] = 0\n",
    "lw_features[\"post_lw\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2011-02-19 al 2011-02-21\n",
    "lw_features.loc['2011-02-18', 'prev_lw'] = 1\n",
    "lw_features.loc['2011-02-19':'2011-02-21', 'lw_type'] = 1\n",
    "lw_features.loc['2011-02-19':'2011-02-21', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2011-02-22', 'post_lw'] = 1\n",
    "\n",
    "# 2011-05-28 al 2011-05-30\n",
    "lw_features.loc['2011-05-27', 'prev_lw'] = 1\n",
    "lw_features.loc['2011-05-28':'2011-05-30', 'lw_type'] = 1\n",
    "lw_features.loc['2011-05-28':'2011-05-30', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2011-05-31', 'post_lw'] = 1\n",
    "\n",
    "# 2011-07-02 al 2011-07-04\n",
    "lw_features.loc['2011-07-01', 'prev_lw'] = 1\n",
    "lw_features.loc['2011-07-02':'2011-07-04', 'lw_type'] = 1\n",
    "lw_features.loc['2011-07-02':'2011-07-04', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2011-07-05', 'post_lw'] = 1\n",
    "\n",
    "# 2011-09-03 al 2011-09-05\n",
    "lw_features.loc['2011-09-02', 'prev_lw'] = 1\n",
    "lw_features.loc['2011-09-03':'2011-09-05', 'lw_type'] = 1\n",
    "lw_features.loc['2011-09-03':'2011-09-05', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2011-09-06', 'post_lw'] = 1\n",
    "\n",
    "# 2011-10-08 al 2011-10-10\n",
    "lw_features.loc['2011-10-07', 'prev_lw'] = 1\n",
    "lw_features.loc['2011-10-08':'2011-10-10', 'lw_type'] = 1\n",
    "lw_features.loc['2011-10-08':'2011-10-10', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2011-10-11', 'post_lw'] = 1\n",
    "\n",
    "# 2011-11-11 al 2011-11-13\n",
    "lw_features.loc['2011-11-10', 'prev_lw'] = 1\n",
    "lw_features.loc['2011-11-11':'2011-11-13', 'lw_type'] = 2\n",
    "lw_features.loc['2011-11-11':'2011-11-13', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2011-11-14', 'post_lw'] = 1\n",
    "\n",
    "# 2011-12-24 al 2011-12-26\n",
    "lw_features.loc['2011-12-23', 'prev_lw'] = 1\n",
    "lw_features.loc['2011-12-24':'2011-12-26', 'lw_type'] = 1\n",
    "lw_features.loc['2011-12-24':'2011-12-26', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2011-12-27', 'post_lw'] = 1\n",
    "\n",
    "# 2011-12-31 al 2012-01-02\n",
    "lw_features.loc['2011-12-30', 'prev_lw'] = 1\n",
    "lw_features.loc['2011-12-31':'2012-01-02', 'lw_type'] = 1\n",
    "lw_features.loc['2011-12-31':'2012-01-02', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2012-01-03', 'post_lw'] = 1\n",
    "\n",
    "# 2012-01-14 al 2012-01-16\n",
    "lw_features.loc['2012-01-13', 'prev_lw'] = 1\n",
    "lw_features.loc['2012-01-14':'2012-01-16', 'lw_type'] = 1\n",
    "lw_features.loc['2012-01-14':'2012-01-16', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2012-01-17', 'post_lw'] = 1\n",
    "\n",
    "# 2012-02-18 al 2012-02-20\n",
    "lw_features.loc['2012-02-17', 'prev_lw'] = 1\n",
    "lw_features.loc['2012-02-18':'2012-02-20', 'lw_type'] = 1\n",
    "lw_features.loc['2012-02-18':'2012-02-20', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2012-02-21', 'post_lw'] = 1\n",
    "\n",
    "# 2012-05-26 al 2012-05-28\n",
    "lw_features.loc['2012-05-25', 'prev_lw'] = 1\n",
    "lw_features.loc['2012-05-26':'2012-05-28', 'lw_type'] = 1\n",
    "lw_features.loc['2012-05-26':'2012-05-28', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2012-05-29', 'post_lw'] = 1\n",
    "\n",
    "# 2012-09-01 al 2012-09-03\n",
    "lw_features.loc['2012-08-31', 'prev_lw'] = 1\n",
    "lw_features.loc['2012-09-01':'2012-09-03', 'lw_type'] = 1\n",
    "lw_features.loc['2012-09-01':'2012-09-03', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2012-09-04', 'post_lw'] = 1\n",
    "\n",
    "# 2012-10-06 al 2012-10-08\n",
    "lw_features.loc['2012-10-05', 'prev_lw'] = 1\n",
    "lw_features.loc['2012-10-06':'2012-10-08', 'lw_type'] = 1\n",
    "lw_features.loc['2012-10-06':'2012-10-08', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2012-10-09', 'post_lw'] = 1\n",
    "\n",
    "# 2013-01-19 al 2013-01-21\n",
    "lw_features.loc['2013-01-18', 'prev_lw'] = 1\n",
    "lw_features.loc['2013-01-19':'2013-01-21', 'lw_type'] = 1\n",
    "lw_features.loc['2013-01-19':'2013-01-21', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2013-01-22', 'post_lw'] = 1\n",
    "\n",
    "# 2013-02-16 al 2013-02-18\n",
    "lw_features.loc['2013-02-15', 'prev_lw'] = 1\n",
    "lw_features.loc['2013-02-16':'2013-02-18', 'lw_type'] = 1\n",
    "lw_features.loc['2013-02-16':'2013-02-18', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2013-02-19', 'post_lw'] = 1\n",
    "\n",
    "# 2013-05-25 al 2013-05-27\n",
    "lw_features.loc['2013-05-24', 'prev_lw'] = 1\n",
    "lw_features.loc['2013-05-25':'2013-05-27', 'lw_type'] = 1\n",
    "lw_features.loc['2013-05-25':'2013-05-27', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2013-05-28', 'post_lw'] = 1\n",
    "\n",
    "# 2013-08-31 al 2013-09-02\n",
    "lw_features.loc['2013-08-30', 'prev_lw'] = 1\n",
    "lw_features.loc['2013-08-31':'2013-09-02', 'lw_type'] = 1\n",
    "lw_features.loc['2013-08-31':'2013-09-02', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2013-09-03', 'post_lw'] = 1\n",
    "\n",
    "# 2013-10-12 al 2013-10-14\n",
    "lw_features.loc['2013-10-11', 'prev_lw'] = 1\n",
    "lw_features.loc['2013-10-12':'2013-10-14', 'lw_type'] = 1\n",
    "lw_features.loc['2013-10-12':'2013-10-14', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2013-10-15', 'post_lw'] = 1\n",
    "\n",
    "# 2013-11-09 al 2013-11-11\n",
    "lw_features.loc['2013-11-08', 'prev_lw'] = 1\n",
    "lw_features.loc['2013-11-09':'2013-11-11', 'lw_type'] = 1\n",
    "lw_features.loc['2013-11-09':'2013-11-11', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2013-11-12', 'post_lw'] = 1\n",
    "\n",
    "# 2014-01-18 al 2014-01-20\n",
    "lw_features.loc['2014-01-17', 'prev_lw'] = 1\n",
    "lw_features.loc['2014-01-18':'2014-01-20', 'lw_type'] = 1\n",
    "lw_features.loc['2014-01-18':'2014-01-20', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2014-01-21', 'post_lw'] = 1\n",
    "\n",
    "# 2014-02-15 al 2014-02-17\n",
    "lw_features.loc['2014-02-14', 'prev_lw'] = 1\n",
    "lw_features.loc['2014-02-15':'2014-02-17', 'lw_type'] = 1\n",
    "lw_features.loc['2014-02-15':'2014-02-17', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2014-02-18', 'post_lw'] = 1\n",
    "\n",
    "# 2014-05-24 al 2014-05-26\n",
    "lw_features.loc['2014-05-23', 'prev_lw'] = 1\n",
    "lw_features.loc['2014-05-24':'2014-05-26', 'lw_type'] = 1\n",
    "lw_features.loc['2014-05-24':'2014-05-26', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2014-05-27', 'post_lw'] = 1\n",
    "\n",
    "# 2014-07-04 al 2014-07-06\n",
    "lw_features.loc['2014-07-03', 'prev_lw'] = 1\n",
    "lw_features.loc['2014-07-04':'2014-07-06', 'lw_type'] = 2\n",
    "lw_features.loc['2014-07-04':'2014-07-06', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2014-07-07', 'post_lw'] = 1\n",
    "\n",
    "# 2014-08-30 al 2014-09-01\n",
    "lw_features.loc['2014-08-29', 'prev_lw'] = 1\n",
    "lw_features.loc['2014-08-30':'2014-09-01', 'lw_type'] = 1\n",
    "lw_features.loc['2014-08-30':'2014-09-01', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2014-09-02', 'post_lw'] = 1\n",
    "\n",
    "# 2014-10-11 al 2014-10-13\n",
    "lw_features.loc['2014-10-10', 'prev_lw'] = 1\n",
    "lw_features.loc['2014-10-11':'2014-10-13', 'lw_type'] = 1\n",
    "lw_features.loc['2014-10-11':'2014-10-13', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2014-10-14', 'post_lw'] = 1\n",
    "\n",
    "# 2015-01-17 al 2015-01-19\n",
    "lw_features.loc['2015-01-16', 'prev_lw'] = 1\n",
    "lw_features.loc['2015-01-17':'2015-01-19', 'lw_type'] = 1\n",
    "lw_features.loc['2015-01-17':'2015-01-19', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2015-01-20', 'post_lw'] = 1\n",
    "\n",
    "# 2015-02-14 al 2015-02-16\n",
    "lw_features.loc['2015-02-13', 'prev_lw'] = 1\n",
    "lw_features.loc['2015-02-14':'2015-02-16', 'lw_type'] = 1\n",
    "lw_features.loc['2015-02-14':'2015-02-16', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2015-02-17', 'post_lw'] = 1\n",
    "\n",
    "# 2015-05-23 al 2015-05-25\n",
    "lw_features.loc['2015-05-22', 'prev_lw'] = 1\n",
    "lw_features.loc['2015-05-23':'2015-05-25', 'lw_type'] = 1\n",
    "lw_features.loc['2015-05-23':'2015-05-25', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2015-05-26', 'post_lw'] = 1\n",
    "\n",
    "# 2015-07-03 al 2015-07-05\n",
    "lw_features.loc['2015-07-02', 'prev_lw'] = 1\n",
    "lw_features.loc['2015-07-03':'2015-07-05', 'lw_type'] = 2\n",
    "lw_features.loc['2015-07-03':'2015-07-05', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2015-07-06', 'post_lw'] = 1\n",
    "\n",
    "# 2015-09-05 al 2015-09-07\n",
    "lw_features.loc['2015-09-04', 'prev_lw'] = 1\n",
    "lw_features.loc['2015-09-05':'2015-09-07', 'lw_type'] = 1\n",
    "lw_features.loc['2015-09-05':'2015-09-07', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2015-09-08', 'post_lw'] = 1\n",
    "\n",
    "# 2015-10-10 al 2015-10-12\n",
    "lw_features.loc['2015-10-09', 'prev_lw'] = 1\n",
    "lw_features.loc['2015-10-10':'2015-10-12', 'lw_type'] = 1\n",
    "lw_features.loc['2015-10-10':'2015-10-12', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2015-10-13', 'post_lw'] = 1\n",
    "\n",
    "# 2015-12-25 al 2015-12-27\n",
    "lw_features.loc['2015-12-24', 'prev_lw'] = 1\n",
    "lw_features.loc['2015-12-25':'2015-12-27', 'lw_type'] = 2\n",
    "lw_features.loc['2015-12-25':'2015-12-27', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2015-12-28', 'post_lw'] = 1\n",
    "\n",
    "# 2016-01-01 al 2016-01-03\n",
    "lw_features.loc['2015-12-31', 'prev_lw'] = 1\n",
    "lw_features.loc['2016-01-01':'2016-01-03', 'lw_type'] = 2\n",
    "lw_features.loc['2016-01-01':'2016-01-03', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2016-01-04', 'post_lw'] = 1\n",
    "\n",
    "# 2016-01-16 al 2016-01-18\n",
    "lw_features.loc['2016-01-15', 'prev_lw'] = 1\n",
    "lw_features.loc['2016-01-16':'2016-01-18', 'lw_type'] = 1\n",
    "lw_features.loc['2016-01-16':'2016-01-18', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2016-01-19', 'post_lw'] = 1\n",
    "\n",
    "# 2016-02-13 al 2016-02-15\n",
    "lw_features.loc['2016-02-12', 'prev_lw'] = 1\n",
    "lw_features.loc['2016-02-13':'2016-02-15', 'lw_type'] = 1\n",
    "lw_features.loc['2016-02-13':'2016-02-15', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2016-02-16', 'post_lw'] = 1\n",
    "\n",
    "# 2016-05-28 al 2016-05-30\n",
    "lw_features.loc['2016-05-27', 'prev_lw'] = 1\n",
    "lw_features.loc['2016-05-28':'2016-05-30', 'lw_type'] = 1\n",
    "lw_features.loc['2016-05-28':'2016-05-30', 'lw_day'] = [1,2,3]\n",
    "lw_features.loc['2016-05-31', 'post_lw'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw_features.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lw_features = reduce_mem_usage(lw_features)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### events features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_features = make_time_range(\"2011-01-29\", \"2016-06-19\", \"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# christmas features\n",
    "events_features[\"prev_christmas\"] = 0\n",
    "events_features.loc[events_features.query(\"ds.dt.month == 12 & ds.dt.day == 20\").index, \"prev_christmas\"] = 1\n",
    "events_features.loc[events_features.query(\"ds.dt.month == 12 & ds.dt.day == 21\").index, \"prev_christmas\"] = 2\n",
    "events_features.loc[events_features.query(\"ds.dt.month == 12 & ds.dt.day == 22\").index, \"prev_christmas\"] = 3\n",
    "events_features.loc[events_features.query(\"ds.dt.month == 12 & ds.dt.day == 23\").index, \"prev_christmas\"] = 4\n",
    "events_features.loc[events_features.query(\"ds.dt.month == 12 & ds.dt.day == 24\").index, \"prev_christmas\"] = 5\n",
    "events_features.loc[events_features.query(\"ds.dt.month == 12 & ds.dt.day == 25\").index, \"prev_christmas\"] = 6\n",
    "\n",
    "events_features[\"post_christmas\"] = 0\n",
    "events_features.loc[events_features.query(\"ds.dt.month == 12 & ds.dt.day == 26\").index, \"post_christmas\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new year features\n",
    "events_features[\"prev_newyear\"] = 0\n",
    "events_features.loc[events_features.query(\"ds.dt.month == 12 & ds.dt.day == 28\").index, \"prev_newyear\"] = 1\n",
    "events_features.loc[events_features.query(\"ds.dt.month == 12 & ds.dt.day == 29\").index, \"prev_newyear\"] = 2\n",
    "events_features.loc[events_features.query(\"ds.dt.month == 12 & ds.dt.day == 30\").index, \"prev_newyear\"] = 3\n",
    "events_features.loc[events_features.query(\"ds.dt.month == 12 & ds.dt.day == 31\").index, \"prev_newyear\"] = 4\n",
    "\n",
    "events_features[\"post_newyear\"] = 0\n",
    "events_features.loc[events_features.query(\"ds.dt.month == 1 & ds.dt.day == 1\").index, \"post_newyear\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanksgiving features\n",
    "events_features[\"prev_thanksgiving\"] = 0\n",
    "events_features[\"post_thanksgiving\"] = 0\n",
    "\n",
    "idx = events_features.query(\"ds.dt.year == 2011 & ds.dt.month == 11 & ds.dt.day in [22,23,24]\").index\n",
    "events_features.loc[idx, \"prev_thanksgiving\"] = [1,2,3]\n",
    "idx = events_features.query(\"ds.dt.year == 2011 & ds.dt.month == 11 & ds.dt.day == 25\").index\n",
    "events_features.loc[idx, \"post_thanksgiving\"] = 1\n",
    "\n",
    "idx = events_features.query(\"ds.dt.year == 2012 & ds.dt.month == 11 & ds.dt.day in [20,21,22]\").index\n",
    "events_features.loc[idx, \"prev_thanksgiving\"] = [1,2,3]\n",
    "idx = events_features.query(\"ds.dt.year == 2012 & ds.dt.month == 11 & ds.dt.day == 23\").index\n",
    "events_features.loc[idx, \"post_thanksgiving\"] = 1\n",
    "\n",
    "idx = events_features.query(\"ds.dt.year == 2013 & ds.dt.month == 11 & ds.dt.day in [26,27,28]\").index\n",
    "events_features.loc[idx, \"prev_thanksgiving\"] = [1,2,3]\n",
    "idx = events_features.query(\"ds.dt.year == 2013 & ds.dt.month == 11 & ds.dt.day == 29\").index\n",
    "events_features.loc[idx, \"post_thanksgiving\"] = 1\n",
    "\n",
    "idx = events_features.query(\"ds.dt.year == 2014 & ds.dt.month == 11 & ds.dt.day in [25,26,27]\").index\n",
    "events_features.loc[idx, \"prev_thanksgiving\"] = [1,2,3]\n",
    "idx = events_features.query(\"ds.dt.year == 2014 & ds.dt.month == 11 & ds.dt.day == 28\").index\n",
    "events_features.loc[idx, \"post_thanksgiving\"] = 1\n",
    "\n",
    "idx = events_features.query(\"ds.dt.year == 2015 & ds.dt.month == 11 & ds.dt.day in [24,25,26]\").index\n",
    "events_features.loc[idx, \"prev_thanksgiving\"] = [1,2,3]\n",
    "idx = events_features.query(\"ds.dt.year == 2015 & ds.dt.month == 11 & ds.dt.day == 27\").index\n",
    "events_features.loc[idx, \"post_thanksgiving\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_features = reduce_mem_usage(events_features)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## extra snap features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar[\"week\"] = calendar.date.dt.weekofyear\n",
    "cumulative_snap = calendar.groupby([\"year\",\"month\",\"week\"])[[\"snap_CA\", \"snap_WI\", \"snap_TX\"]].cumsum()\n",
    "cumulative_snap.columns = [f\"{col}_cum\" for col in cumulative_snap.columns]\n",
    "calendar.drop(\"week\", axis=1, inplace=True)\n",
    "for column in cumulative_snap.columns:\n",
    "    calendar[column] = cumulative_snap[column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train[\"id\"] = sales_train.id.map(lambda x: x.replace(\"_evaluation\", \"\"))\n",
    "hierarchy = (sales_train.loc[:, [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]]\n",
    "             .drop_duplicates())\n",
    "encoders = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy.to_parquet(\"../input/hierarchy_raw.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchy encoder\n",
    "id_encoder = OrdinalEncoder()\n",
    "id_encoder.fit(hierarchy.loc[:, [\"id\"]])\n",
    "hierarchy[\"ts_id\"]  = id_encoder.transform(hierarchy.loc[:, [\"id\"]])\n",
    "encoders[\"id\"] = id_encoder\n",
    "\n",
    "item_encoder = OrdinalEncoder()\n",
    "item_encoder.fit(hierarchy.loc[:, [\"item_id\"]])\n",
    "hierarchy.loc[:, \"item_id\"]  = item_encoder.transform(hierarchy.loc[:, [\"item_id\"]])\n",
    "encoders[\"item\"] = item_encoder\n",
    "\n",
    "dept_encoder = OrdinalEncoder()\n",
    "dept_encoder.fit(hierarchy.loc[:, [\"dept_id\"]])\n",
    "hierarchy.loc[:, \"dept_id\"]  = dept_encoder.transform(hierarchy.loc[:, [\"dept_id\"]])\n",
    "encoders[\"dept\"] = dept_encoder\n",
    "\n",
    "cat_encoder = OrdinalEncoder()\n",
    "cat_encoder.fit(hierarchy.loc[:, [\"cat_id\"]])\n",
    "hierarchy.loc[:, \"cat_id\"]   = cat_encoder.transform(hierarchy.loc[:, [\"cat_id\"]])\n",
    "encoders[\"cat\"] = cat_encoder\n",
    "\n",
    "store_encoder = OrdinalEncoder()\n",
    "store_encoder.fit(hierarchy.loc[:, [\"store_id\"]])\n",
    "hierarchy.loc[:, \"store_id\"] = store_encoder.transform(hierarchy.loc[:, [\"store_id\"]])\n",
    "encoders[\"store\"] = store_encoder\n",
    "\n",
    "state_encoder = OrdinalEncoder()\n",
    "state_encoder.fit(hierarchy.loc[:, [\"state_id\"]])\n",
    "hierarchy.loc[:, \"state_id\"] = state_encoder.transform(hierarchy.loc[:, [\"state_id\"]])\n",
    "encoders[\"state\"] = state_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy.to_parquet(\"../input/hierarchy.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"../input/encoders.pkl\", \"wb\")\n",
    "pickle.dump(encoders, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## calendar events encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_name_1_encoder = OrdinalEncoder()\n",
    "event_name_1_encoder.fit(calendar.loc[:, [\"event_name_1\"]])\n",
    "calendar.loc[:, \"event_name_1\"] = event_name_1_encoder.transform(calendar.loc[:, [\"event_name_1\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type_1_encoder = OrdinalEncoder()\n",
    "event_type_1_encoder.fit(calendar.loc[:, [\"event_type_1\"]])\n",
    "calendar.loc[:, \"event_type_1\"] = event_type_1_encoder.transform(calendar.loc[:, [\"event_type_1\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_name_2_encoder = OrdinalEncoder()\n",
    "event_name_2_encoder.fit(calendar.loc[:, [\"event_name_2\"]])\n",
    "calendar.loc[:, \"event_name_2\"] = event_name_2_encoder.transform(calendar.loc[:, [\"event_name_2\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type_2_encoder = OrdinalEncoder()\n",
    "event_type_2_encoder.fit(calendar.loc[:, [\"event_type_2\"]])\n",
    "calendar.loc[:, \"event_type_2\"] = event_type_2_encoder.transform(calendar.loc[:, [\"event_type_2\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar = reduce_mem_usage(calendar)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train[\"ts_id\"] = id_encoder.transform(sales_train.loc[:, [\"id\"]])\n",
    "sales_train.loc[:, \"item_id\"]  = item_encoder.transform(sales_train.loc[:, [\"item_id\"]])\n",
    "sales_train.loc[:, \"dept_id\"]  = dept_encoder.transform(sales_train.loc[:, [\"dept_id\"]])\n",
    "sales_train.loc[:, \"cat_id\"]   = cat_encoder.transform(sales_train.loc[:, [\"cat_id\"]])\n",
    "sales_train.loc[:, \"store_id\"] = store_encoder.transform(sales_train.loc[:, [\"store_id\"]])\n",
    "sales_train.loc[:, \"state_id\"] = state_encoder.transform(sales_train.loc[:, [\"state_id\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices.loc[:, \"store_id\"] = store_encoder.transform(sell_prices.loc[:, [\"store_id\"]])\n",
    "sell_prices.loc[:, \"item_id\"]  = item_encoder.transform(sell_prices.loc[:, [\"item_id\"]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## cleaning of price values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11325</td>\n",
       "      <td>9.58</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11326</td>\n",
       "      <td>9.58</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11327</td>\n",
       "      <td>8.26</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11328</td>\n",
       "      <td>8.26</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11329</td>\n",
       "      <td>8.26</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_id  item_id  wm_yr_wk  sell_price    q\n",
       "0         1        1     11325        9.58  1.0\n",
       "1         1        1     11326        9.58  0.0\n",
       "2         1        1     11327        8.26  2.0\n",
       "3         1        1     11328        8.26  2.0\n",
       "4         1        1     11329        8.26  6.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data = pd.melt(sales_train, \n",
    "                id_vars=[\"ts_id\",\"item_id\",\"store_id\"],\n",
    "                value_vars=[f\"d_{i}\" for i in range(1,1914)],\n",
    "                var_name=\"d\",\n",
    "                value_name=\"q\")\n",
    "\n",
    "_data = pd.merge(_data, \n",
    "                 calendar.loc[:, [\"d\",\"wm_yr_wk\"]],\n",
    "                 how=\"left\",\n",
    "                 on=\"d\")\n",
    "\n",
    "sales_weekly = _data.groupby([\"item_id\",\"store_id\",\"wm_yr_wk\"])[\"q\"].sum().reset_index()\n",
    "sell_prices = pd.merge(sell_prices, sales_weekly, how=\"left\", on=[\"store_id\",\"item_id\",\"wm_yr_wk\"])\n",
    "sell_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prices(df, kernel_size=7):\n",
    "    df = df.copy()\n",
    "    df.rename({\"sell_price\":\"sell_price_raw\"}, axis=1, inplace=True)\n",
    "    df[\"sell_price\"] = medfilt(df.sell_price_raw, kernel_size=kernel_size)\n",
    "    q75 = df.q.quantile(0.75)\n",
    "    q25 = df.q.quantile(0.25)\n",
    "    max_week = df.dropna().wm_yr_wk.max()\n",
    "    keep1_idx = df.query(\"sell_price_raw < sell_price & q > @q75 & wm_yr_wk < @max_week\").index\n",
    "    keep2_idx = df.query(\"sell_price_raw > sell_price & q < @q25 & wm_yr_wk < @max_week\").index\n",
    "    df.loc[keep1_idx, \"sell_price\"] = df.loc[keep1_idx, \"sell_price_raw\"]\n",
    "    df.loc[keep2_idx, \"sell_price\"] = df.loc[keep2_idx, \"sell_price_raw\"]\n",
    "    df.drop(\"sell_price_raw\", axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 29s, sys: 7.12 s, total: 7min 36s\n",
      "Wall time: 7min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sell_prices = sell_prices.groupby([\"item_id\",\"store_id\"]).apply(clean_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices.reset_index(drop=True, inplace=True)\n",
    "sell_prices.drop(\"q\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## building price features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_prices = (sell_prices\n",
    "                 .groupby([\"store_id\", \"item_id\"])[\"sell_price\"]\n",
    "                 .apply(lambda x: len(np.unique(x)))\n",
    "                 .reset_index(name=\"n_prices\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    30490.000000\n",
       "mean         2.769367\n",
       "std          1.893627\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          2.000000\n",
       "75%          4.000000\n",
       "max         21.000000\n",
       "Name: n_prices, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_prices.n_prices.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifies the more frequent price\n",
    "regular_prices = (\n",
    "    sell_prices\n",
    "    .groupby([\"store_id\", \"item_id\"])[\"sell_price\"]\n",
    "    .apply(lambda x: x.value_counts().index[0])\n",
    "    .reset_index(name=\"regular_price\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price variation\n",
    "price_variation1 = (sell_prices.groupby([\"store_id\",\"item_id\"])[\"sell_price\"].quantile(0.75) - \n",
    "                    sell_prices.groupby([\"store_id\",\"item_id\"])[\"sell_price\"].quantile(0.25)).reset_index(name=\"price_iqr1\")\n",
    "\n",
    "price_variation2 = (sell_prices.groupby([\"store_id\",\"item_id\"])[\"sell_price\"].quantile(0.95) - \n",
    "                    sell_prices.groupby([\"store_id\",\"item_id\"])[\"sell_price\"].quantile(0.05)).reset_index(name=\"price_iqr2\")\n",
    "\n",
    "# min and max prices\n",
    "price_min = sell_prices.groupby([\"store_id\",\"item_id\"])[\"sell_price\"].quantile(0.05).reset_index(name=\"price_min\")\n",
    "price_max = sell_prices.groupby([\"store_id\",\"item_id\"])[\"sell_price\"].quantile(0.95).reset_index(name=\"price_max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices = (\n",
    "    sell_prices\n",
    "    .merge(number_prices, how=\"left\")\n",
    "    .merge(regular_prices, how=\"left\")\n",
    "    .merge(price_variation1, how=\"left\")\n",
    "    .merge(price_variation2, how=\"left\")\n",
    "    .merge(price_min, how=\"left\")\n",
    "    .merge(price_max, how=\"left\")\n",
    "    .assign(discount = lambda x: (x.regular_price - x.sell_price))\n",
    "    .assign(discount_norm = lambda x: (x.regular_price - x.sell_price)/x.regular_price)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices = sell_prices.merge(calendar.loc[:, ['wm_yr_wk',\"date\"]].drop_duplicates(subset=[\"wm_yr_wk\"]), \n",
    "                                on=['wm_yr_wk'], \n",
    "                                how='left')\n",
    "\n",
    "sell_prices[\"month\"] = sell_prices.date.dt.month\n",
    "sell_prices[\"quarter\"] = sell_prices.date.dt.quarter\n",
    "sell_prices[\"semester\"] = None\n",
    "sell_prices.loc[sell_prices.query(\"quarter in [1,2]\").index, \"semester\"] = 1\n",
    "sell_prices.loc[sell_prices.query(\"quarter in [3,4]\").index, \"semester\"] = 2\n",
    "sell_prices[\"year\"] = sell_prices.date.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# price momentum\n",
    "sell_prices['price_momentum_m'] = sell_prices['sell_price']/sell_prices.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
    "#sell_prices['price_momentum_q'] = sell_prices['sell_price']/sell_prices.groupby(['store_id','item_id','quarter'])['sell_price'].transform('mean')\n",
    "#sell_prices['price_momentum_s'] = sell_prices['sell_price']/sell_prices.groupby(['store_id','item_id','semester'])['sell_price'].transform('mean')\n",
    "sell_prices['price_momentum_y'] = sell_prices['sell_price']/sell_prices.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n",
    "sell_prices.drop([\"year\",\"semester\",\"quarter\",\"month\",\"date\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sell_prices.groupby(['store_id','item_id','month'])['sell_price'].transform(lambda x: x.value_counts().index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifies the more frequent price\n",
    "#regular_prices = (\n",
    "#    sell_prices\n",
    "#    .groupby([\"store_id\", \"item_id\"])[\"sell_price\"]\n",
    "#    .apply(lambda x: x.value_counts().index[0])\n",
    "#    .reset_index(name=\"regular_price\")\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(sales_train, \n",
    "               id_vars=[\"ts_id\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\"],\n",
    "               value_vars=[f\"d_{i}\" for i in range(1,1942)],\n",
    "               var_name=\"d\",\n",
    "               value_name=\"q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_columns = [\"date\", \"wm_yr_wk\", \"d\", \"snap_CA\", \"snap_TX\", \"snap_WI\",\n",
    "                    \"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n",
    "data = pd.merge(data, \n",
    "                calendar.loc[:, calendar_columns],\n",
    "                how=\"left\",\n",
    "                on=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, sell_prices,\n",
    "                on=[\"store_id\", \"item_id\", \"wm_yr_wk\"],\n",
    "                how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename({\"date\":\"ds\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (data\n",
    "        .merge(events_features, how=\"left\", on=[\"ds\"])\n",
    "        .merge(lw_features, how=\"left\", on=[\"ds\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data.sort_values([\"item_id\",\"store_id\",\"ds\"], inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### removes zeros at the start of the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.distplot(data.query(\"q <= 20\").q.values, kde=False)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_starting_zeros(dataframe):\n",
    "    idxmin = dataframe.query(\"q > 0\").index.min()\n",
    "    return dataframe.loc[idxmin:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = (data\n",
    "        .groupby([\"item_id\",\"store_id\"])\n",
    "        .apply(remove_starting_zeros)\n",
    "        .reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.distplot(data.query(\"q <= 20\").q.values, kde=False)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## out of stock patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_out_of_stock(df, thresholds=[(28,56),(56,84),(84,112),(112,140),(140,168),(168,2000)]):\n",
    "    df = df.copy()\n",
    "    df[\"no_stock\"] = 0\n",
    "    zero_mask = (df.q == 0)\n",
    "    transition_mask = (zero_mask != zero_mask.shift(1))\n",
    "    zero_sequences = transition_mask.cumsum()[zero_mask]\n",
    "    zero_seqs_count = zero_sequences.map(zero_sequences.value_counts()).to_frame()\n",
    "    for i,values in enumerate(thresholds):\n",
    "        left_thresh,right_thresh = values\n",
    "        idx = zero_seqs_count.query(\"@left_thresh <= q < @right_thresh\").index \n",
    "        df.loc[idx, \"no_stock\"] = i+1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = data.groupby([\"item_id\",\"store_id\"]).apply(find_out_of_stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True, inplace=True)\n",
    "data.drop([\"d\", \"wm_yr_wk\"], axis=1, inplace=True)\n",
    "data = reduce_mem_usage(data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the ts has no sales in the past 8 weeks, will be marked as out-of-stock for the prediction period.\n",
    "no_stock_ts = list()\n",
    "for threshold in [28, 56, 84, 112, 140, 168]:\n",
    "    left_date = data.ds.max() - pd.DateOffset(days=threshold)\n",
    "    no_stock_ts.append((data\n",
    "                        .query(\"ds >= @left_date\")\n",
    "                        .groupby([\"ts_id\"])\n",
    "                        .filter(lambda x: np.all(x.q==0))\n",
    "                        .loc[:, [\"ts_id\"]]\n",
    "                        .drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "evaluation dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calendar_columns = [\"date\", \"wm_yr_wk\", \"snap_CA\", \"snap_TX\", \"snap_WI\",\n",
    "#                    \"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n",
    "\n",
    "#valid_dataframe = (pd.concat([make_time_range(\"2016-04-25\", \"2016-05-22\", \"D\").assign(**row)\n",
    "#                              for _,row in hierarchy.iterrows()], ignore_index=True)\n",
    "#                   .merge(calendar.loc[:, calendar_columns],\n",
    "#                          how=\"left\", left_on=\"ds\", right_on=\"date\")\n",
    "#                   .merge(sell_prices, how=\"left\")\n",
    "#                   .merge(events_features, how=\"left\", on=\"ds\")\n",
    "#                   .merge(lw_features, how=\"left\", on=\"ds\")\n",
    "#                   .drop([\"id\",\"date\",\"wm_yr_wk\"], axis=1)\n",
    "#                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_dataframe[\"snap\"] = 0\n",
    "\n",
    "#idx_snap_ca = valid_dataframe.query(\"state_id==1 & snap_CA==1\").index\n",
    "#valid_dataframe.loc[idx_snap_ca, \"snap\"] = 1\n",
    "\n",
    "#idx_snap_tx = valid_dataframe.query(\"state_id==2 & snap_TX==1\").index\n",
    "#valid_dataframe.loc[idx_snap_tx, \"snap\"] = 1\n",
    "\n",
    "#idx_snap_wi = valid_dataframe.query(\"state_id==3 & snap_WI==1\").index\n",
    "#valid_dataframe.loc[idx_snap_wi, \"snap\"] = 1\n",
    "\n",
    "#valid_dataframe.drop([\"snap_CA\", \"snap_TX\", \"snap_WI\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_dataframe[\"no_stock\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_dataframe = reduce_mem_usage(valid_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_columns = [\"date\", \"wm_yr_wk\", \"snap_CA\", \"snap_TX\", \"snap_WI\",\n",
    "                    \"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n",
    "\n",
    "eval_dataframe = (pd.concat([make_time_range(\"2016-05-23\", \"2016-06-19\", \"D\").assign(**row)\n",
    "                             for _,row in hierarchy.iterrows()], ignore_index=True)\n",
    "                  .merge(calendar.loc[:, calendar_columns],\n",
    "                         how=\"left\", left_on=\"ds\", right_on=\"date\")\n",
    "                  .merge(sell_prices, how=\"left\")\n",
    "                  .merge(events_features, how=\"left\", on=\"ds\")\n",
    "                  .merge(lw_features, how=\"left\", on=\"ds\")\n",
    "                  .drop([\"id\",\"date\",\"wm_yr_wk\"], axis=1)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataframe[\"snap\"] = 0\n",
    "\n",
    "idx_snap_ca = eval_dataframe.query(\"state_id==1 & snap_CA==1\").index\n",
    "eval_dataframe.loc[idx_snap_ca, \"snap\"] = 1\n",
    "\n",
    "idx_snap_tx = eval_dataframe.query(\"state_id==2 & snap_TX==1\").index\n",
    "eval_dataframe.loc[idx_snap_tx, \"snap\"] = 1\n",
    "\n",
    "idx_snap_wi = eval_dataframe.query(\"state_id==3 & snap_WI==1\").index\n",
    "eval_dataframe.loc[idx_snap_wi, \"snap\"] = 1\n",
    "\n",
    "eval_dataframe.drop([\"snap_CA\", \"snap_TX\", \"snap_WI\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataframe[\"no_stock\"] = 0\n",
    "\n",
    "for i,no_stock in enumerate(no_stock_ts):\n",
    "    idx = eval_dataframe.query(\"ts_id in @no_stock.ts_id\").index\n",
    "    eval_dataframe.loc[idx, \"no_stock\"] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataframe = reduce_mem_usage(eval_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Saving the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "(data\n",
    " .to_parquet(\"../input/train_dataframe.parquet\", index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation data\n",
    "(eval_dataframe\n",
    " .to_parquet(\"../input/eval_dataframe.parquet\", index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
