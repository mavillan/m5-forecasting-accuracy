{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/turing/miniconda/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from tsforest.utils import make_time_range\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# local modules\n",
    "import sys\n",
    "sys.path.append(\"../lib/\")\n",
    "from utils import compute_scaling, reduce_mem_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30490 entries, 0 to 30489\n",
      "Columns: 1947 entries, id to d_1941\n",
      "dtypes: int64(1941), object(6)\n",
      "memory usage: 452.9+ MB\n"
     ]
    }
   ],
   "source": [
    "sales_train = pd.read_csv(\"../input/sales_train_evaluation.csv\")\n",
    "sales_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1969 entries, 0 to 1968\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   date          1969 non-null   datetime64[ns]\n",
      " 1   wm_yr_wk      1969 non-null   int64         \n",
      " 2   weekday       1969 non-null   object        \n",
      " 3   wday          1969 non-null   int64         \n",
      " 4   month         1969 non-null   int64         \n",
      " 5   year          1969 non-null   int64         \n",
      " 6   d             1969 non-null   object        \n",
      " 7   event_name_1  162 non-null    object        \n",
      " 8   event_type_1  162 non-null    object        \n",
      " 9   event_name_2  5 non-null      object        \n",
      " 10  event_type_2  5 non-null      object        \n",
      " 11  snap_CA       1969 non-null   int64         \n",
      " 12  snap_TX       1969 non-null   int64         \n",
      " 13  snap_WI       1969 non-null   int64         \n",
      "dtypes: datetime64[ns](1), int64(7), object(6)\n",
      "memory usage: 215.5+ KB\n"
     ]
    }
   ],
   "source": [
    "calendar = pd.read_csv(\"../input/calendar.csv\", parse_dates=[\"date\"])\n",
    "calendar.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train[\"id\"] = sales_train.id.map(lambda x: x.replace(\"_evaluation\", \"\"))\n",
    "hierarchy = (sales_train.loc[:, [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]]\n",
    "             .drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchy encoder\n",
    "id_encoder = OrdinalEncoder()\n",
    "id_encoder.fit(hierarchy.loc[:, [\"id\"]])\n",
    "hierarchy[\"ts_id\"]  = id_encoder.transform(hierarchy.loc[:, [\"id\"]])\n",
    "\n",
    "item_encoder = OrdinalEncoder()\n",
    "item_encoder.fit(hierarchy.loc[:, [\"item_id\"]])\n",
    "hierarchy.loc[:, \"item_id\"]  = item_encoder.transform(hierarchy.loc[:, [\"item_id\"]])\n",
    "\n",
    "dept_encoder = OrdinalEncoder()\n",
    "dept_encoder.fit(hierarchy.loc[:, [\"dept_id\"]])\n",
    "hierarchy.loc[:, \"dept_id\"]  = dept_encoder.transform(hierarchy.loc[:, [\"dept_id\"]])\n",
    "\n",
    "cat_encoder = OrdinalEncoder()\n",
    "cat_encoder.fit(hierarchy.loc[:, [\"cat_id\"]])\n",
    "hierarchy.loc[:, \"cat_id\"]   = cat_encoder.transform(hierarchy.loc[:, [\"cat_id\"]])\n",
    "\n",
    "store_encoder = OrdinalEncoder()\n",
    "store_encoder.fit(hierarchy.loc[:, [\"store_id\"]])\n",
    "hierarchy.loc[:, \"store_id\"] = store_encoder.transform(hierarchy.loc[:, [\"store_id\"]])\n",
    "\n",
    "state_encoder = OrdinalEncoder()\n",
    "state_encoder.fit(hierarchy.loc[:, [\"state_id\"]])\n",
    "hierarchy.loc[:, \"state_id\"] = state_encoder.transform(hierarchy.loc[:, [\"state_id\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train[\"ts_id\"] = id_encoder.transform(sales_train.loc[:, [\"id\"]])\n",
    "sales_train.loc[:, \"item_id\"]  = item_encoder.transform(sales_train.loc[:, [\"item_id\"]])\n",
    "sales_train.loc[:, \"dept_id\"]  = dept_encoder.transform(sales_train.loc[:, [\"dept_id\"]])\n",
    "sales_train.loc[:, \"cat_id\"]   = cat_encoder.transform(sales_train.loc[:, [\"cat_id\"]])\n",
    "sales_train.loc[:, \"store_id\"] = store_encoder.transform(sales_train.loc[:, [\"store_id\"]])\n",
    "sales_train.loc[:, \"state_id\"] = state_encoder.transform(sales_train.loc[:, [\"state_id\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(sales_train, \n",
    "               id_vars=[\"ts_id\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\"],\n",
    "               value_vars=[f\"d_{i}\" for i in range(1,1942)],\n",
    "               var_name=\"d\",\n",
    "               value_name=\"q\")\n",
    "data = pd.merge(data, \n",
    "                calendar.loc[:, [\"d\",\"date\"]],\n",
    "                how=\"left\",\n",
    "                on=\"d\")\n",
    "data.drop(\"d\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = reduce_mem_usage(data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 59181090 entries, 0 to 59181089\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Dtype         \n",
      "---  ------    -----         \n",
      " 0   ts_id     int16         \n",
      " 1   item_id   int16         \n",
      " 2   dept_id   int8          \n",
      " 3   cat_id    int8          \n",
      " 4   store_id  int8          \n",
      " 5   state_id  int8          \n",
      " 6   q         int16         \n",
      " 7   date      datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int16(3), int8(4)\n",
      "memory usage: 1.4 GB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### removes zeros at the start of the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_starting_zeros(dataframe):\n",
    "    idxmin = dataframe.query(\"q > 0\").index.min()\n",
    "    return dataframe.loc[idxmin:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (data\n",
    "        .groupby([\"item_id\",\"store_id\"])\n",
    "        .apply(remove_starting_zeros)\n",
    "        .reset_index(drop=True)\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46796220 entries, 0 to 46796219\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Dtype         \n",
      "---  ------    -----         \n",
      " 0   ts_id     int16         \n",
      " 1   item_id   int16         \n",
      " 2   dept_id   int8          \n",
      " 3   cat_id    int8          \n",
      " 4   store_id  int8          \n",
      " 5   state_id  int8          \n",
      " 6   q         int16         \n",
      " 7   date      datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int16(3), int8(4)\n",
      "memory usage: 803.3 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet(\"../input/scaling_input.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Precomputation of scaling for all levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_id_columns_by_level = {\n",
    "    1: [],\n",
    "    2: [\"state_id\"],\n",
    "    3: [\"store_id\"],\n",
    "    4: [\"cat_id\"],\n",
    "    5: [\"dept_id\"],\n",
    "    6: [\"state_id\", \"cat_id\"],\n",
    "    7: [\"state_id\", \"dept_id\"],\n",
    "    8: [\"store_id\", \"cat_id\"],\n",
    "    9: [\"store_id\", \"dept_id\"],\n",
    "    10: [\"item_id\"],\n",
    "    11: [\"item_id\", \"state_id\"],\n",
    "    12: [\"item_id\", \"store_id\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 []\n",
      "2 ['state_id']\n",
      "3 ['store_id']\n",
      "4 ['cat_id']\n",
      "5 ['dept_id']\n",
      "6 ['state_id', 'cat_id']\n",
      "7 ['state_id', 'dept_id']\n",
      "8 ['store_id', 'cat_id']\n",
      "9 ['store_id', 'dept_id']\n",
      "10 ['item_id']\n",
      "11 ['item_id', 'state_id']\n",
      "12 ['item_id', 'store_id']\n"
     ]
    }
   ],
   "source": [
    "for level,ts_uid_columns in ts_id_columns_by_level.items():\n",
    "    print(level, ts_uid_columns)\n",
    "    if level == 1: continue\n",
    "    scales = compute_scaling(data, agg_columns=ts_uid_columns).rename({\"q\":\"s\"}, axis=1)\n",
    "    scales.to_parquet(f\"../input/scales_level{level}.parquet\", index=False)\n",
    "    \n",
    "# scaling factor for root level\n",
    "_data = (data\n",
    "         .groupby([\"date\"])[\"q\"]\n",
    "         .sum()\n",
    "         .reset_index())\n",
    "scales = pd.DataFrame([np.sqrt(np.nanmean(_data.q.diff(1)**2))], columns=[\"s\"])\n",
    "scales.to_parquet(f\"../input/scales_level1.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
