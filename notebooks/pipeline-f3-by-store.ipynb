{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/turing/miniconda/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np; np.random.seed(42)\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import category_encoders as ce\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tsforest.trend import compute_trend_models\n",
    "from tsforest.forecaster import LightGBMForecaster\n",
    "from tsforest.utils import make_time_range\n",
    "from tsforest.metrics import compute_rmse\n",
    "\n",
    "# local modules\n",
    "import sys\n",
    "sys.path.append(\"../lib/\")\n",
    "from utils import compute_scaling, compute_weights, reduce_mem_usage\n",
    "from evaluation import _WRMSSEEvaluator, WRMSSEEvaluator, Evaluator, WRMSSEEvaluatorL12\n",
    "from encoding import HierarchicalEncoder\n",
    "\n",
    "def trimean(array, axis=0):\n",
    "    quantiles = np.percentile(array, [25, 50, 75], axis=axis)\n",
    "    return (quantiles[0,:] + 2*quantiles[1,:] + quantiles[2,:])/4\n",
    "\n",
    "SEEDS = [2,3,5,7,11,13,17,19,23,29,31,37,41,43,47,53,59,61,67,71]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation period to be used for test in all this notebook\n",
    "valid_period = (pd.to_datetime(\"2016-03-28\"), pd.to_datetime(\"2016-04-24\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Level 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (pd.read_parquet(\"../input/train_dataframe.parquet\")\n",
    "        .reset_index(drop=True)\n",
    "        .rename({\"q\":\"y\"}, axis=1)\n",
    "       )\n",
    "data[\"sales\"] = data.eval(\"y * sell_price\")\n",
    "\n",
    "scaling_input = pd.read_parquet(\"../input/scaling_input.parquet\")\n",
    "weighting_input = pd.read_parquet(\"../input/weighting_input.parquet\")\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales_level12 = compute_scaling(scaling_input, \n",
    "                                 cut_date=valid_period[0],\n",
    "                                 agg_columns=[\"item_id\",\"store_id\"]).rename({\"q\":\"s\"}, axis=1)\n",
    "\n",
    "weights_level12 = compute_weights(weighting_input, \n",
    "                                  start_date=valid_period[0], level=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier removal\n",
    "#remove_idx = data.query(\"ds.dt.month == 12 & ds.dt.day == 25\").index\n",
    "#data.drop(remove_idx, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'objective':'tweedie',\n",
    "    'tweedie_variance_power': 1.1,\n",
    "    'metric':'None',\n",
    "    'num_iterations':100000,\n",
    "    'early_stopping_rounds':300,\n",
    "    'max_bin': 127,\n",
    "    'bin_construct_sample_cnt':6000000,\n",
    "    'num_leaves': 2**9-1,\n",
    "    'min_data_in_leaf': 2**9-1,\n",
    "    'learning_rate': 0.03, \n",
    "    #'min_sum_hessian_in_leaf':1e-4,\n",
    "    'feature_fraction': 0.9,\n",
    "    #'feature_fraction_bynode':0.9,\n",
    "    'bagging_fraction':0.66,\n",
    "    'bagging_freq':1,\n",
    "    'lambda_l2':0.1,\n",
    "    'seed':7,\n",
    "    'boost_from_average': False,\n",
    "    'first_metric_only': True,\n",
    "}\n",
    "\n",
    "time_features = [\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    #\"year_week\",\n",
    "    #\"year_day\",\n",
    "    \"week_day\",\n",
    "    \"month_progress\", \n",
    "    #\"week_day_cos\",\n",
    "    #\"week_day_sin\",\n",
    "    #\"year_day_cos\",\n",
    "    #\"year_day_sin\",\n",
    "    \"year_week_cos\",\n",
    "    \"year_week_sin\",\n",
    "    #\"month_cos\",\n",
    "    #\"month_sin\"\n",
    "]\n",
    "\n",
    "exclude_features = [\n",
    "    \"ts_id\",\n",
    "    \"store_id\",\n",
    "    \"state_id\",\n",
    "    \"event_type_1\",\n",
    "    \"event_name_2\",\n",
    "    \"event_type_2\",\n",
    "    \"prev_newyear\",\n",
    "    \"post_newyear\",\n",
    "    \"no_stock_days\",\n",
    "    \"sales\",\n",
    "]\n",
    "\n",
    "model_kwargs = {\n",
    "    \"model_params\":model_params,\n",
    "    \"time_features\":time_features,\n",
    "    \"window_shifts\":[1,7,28,56],\n",
    "    \"window_functions\":[\"mean\",\"median\",\"std\",\"kurt\",],\n",
    "    \"window_sizes\":[7,28],\n",
    "    \"exclude_features\":exclude_features,\n",
    "    \"categorical_features\":{\n",
    "        \"item_id\": (\"y\", ce.GLMMEncoder, None),\n",
    "        \"dept_id\": \"default\",\n",
    "        \"cat_id\": \"default\",\n",
    "        \"event_name_1\": \"default\",\n",
    "    },\n",
    "    \"ts_uid_columns\":[\"item_id\",],\n",
    "}\n",
    "\n",
    "lagged_features = list()\n",
    "if \"lags\" in model_kwargs.keys():\n",
    "    lag_features = [f\"lag{lag}\" for lag in model_kwargs[\"lags\"]]\n",
    "    lagged_features.extend(lag_features)\n",
    "if \"window_functions\" in model_kwargs.keys():\n",
    "    rw_features = [f\"{window_func}{window_size}_shift{window_shift}\" \n",
    "                   for window_func in model_kwargs[\"window_functions\"]\n",
    "                   for window_size in model_kwargs[\"window_sizes\"]\n",
    "                   for window_shift in model_kwargs[\"window_shifts\"]]\n",
    "    lagged_features.extend(rw_features)\n",
    "    \n",
    "lagged_features_to_dropna = [feat for feat in lagged_features if feat not in [\"skew\", \"kurt\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"#\"*100)\n",
    "print(f\" Validation period: {valid_period} \".center(100, \"#\"))\n",
    "print(\"#\"*100)\n",
    "\n",
    "valid_start = valid_period[0]\n",
    "valid_end = valid_period[1]\n",
    "stores_forecast = list()\n",
    "\n",
    "for store_id in range(1,11):\n",
    "    print(\"-\"*100)\n",
    "    print(f\" store_id: {store_id} \".center(100, \"-\"))\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    _train_data = data.query(\"ds <= @valid_end & store_id == @store_id\").reset_index(drop=True)\n",
    "    _valid_index = _train_data.query(\"@valid_start <= ds <= @valid_end\").index\n",
    "\n",
    "    if store_id in [1,2,3,4]:\n",
    "        # CA store\n",
    "        _train_data.drop([\"snap_TX\", \"snap_TX_cum\", \"snap_WI\", \"snap_WI_cum\"], axis=1, inplace=True)\n",
    "    elif store_id in [5,6,7]:\n",
    "        # TX store\n",
    "        _train_data.drop([\"snap_CA\", \"snap_CA_cum\", \"snap_WI\", \"snap_WI_cum\"], axis=1, inplace=True)\n",
    "    else:\n",
    "        #WI store\n",
    "        _train_data.drop([\"snap_TX\", \"snap_TX_cum\", \"snap_CA\", \"snap_CA_cum\"], axis=1, inplace=True)\n",
    "\n",
    "    model_level12 = LightGBMForecaster(**model_kwargs)\n",
    "    model_level12.prepare_features(train_data=_train_data, valid_index=_valid_index)\n",
    "    model_level12.train_features.dropna(subset=lagged_features_to_dropna, axis=0, inplace=True)\n",
    "    model_level12.train_features = reduce_mem_usage(model_level12.train_features)\n",
    "    model_level12.valid_features = reduce_mem_usage(model_level12.valid_features)\n",
    "    ts_id_in_train = model_level12.train_features.ts_id.unique()\n",
    "    model_level12.valid_features = model_level12.valid_features.query(\"ts_id in @ts_id_in_train\")\n",
    "\n",
    "    print(\"Fitting the model\")\n",
    "    tic = time.time()\n",
    "    evaluator = WRMSSEEvaluatorL12(model_level12.valid_features, weights_level12, scales_level12)\n",
    "    model_level12.fit(fit_kwargs={\"verbose_eval\":25, \"feval\":evaluator.evaluate})\n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60.} [min]\")\n",
    "    print(f\"- best_iteration: {model_level12.best_iteration} \\n\\n\")\n",
    "\n",
    "    lgb.plot_importance(model_level12.model.model, importance_type=\"split\", figsize=(10,15))\n",
    "    lgb.plot_importance(model_level12.model.model, importance_type=\"gain\", figsize=(10,15))\n",
    "\n",
    "    print(\"Predicting with ground thruth lagged values\")\n",
    "    tic = time.time()\n",
    "    forecast_v0 = (model_level12.valid_features\n",
    "                   .loc[:, [\"ds\"]+model_level12.ts_uid_columns]\n",
    "                   .assign(y_pred = model_level12.model.predict(model_level12.valid_features)))\n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60.} [min]\")\n",
    "\n",
    "    wrmsse = evaluator._evaluate(forecast_v0.y_pred.values)\n",
    "    print(f\"wrmsse: {wrmsse} \\n\")\n",
    "\n",
    "    print(\"Predicting with recursive approach\")\n",
    "    tic = time.time()\n",
    "    valid_data = model_level12.valid_features.loc[:, model_level12.raw_train_columns].drop(\"y\", axis=1)\n",
    "    forecast_v1 = model_level12.predict(valid_data, recursive=True)\n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60.} [min]\")\n",
    "\n",
    "    wrmsse = evaluator._evaluate(forecast_v1.y_pred.values)\n",
    "    print(f\"wrmsse: {wrmsse} \\n\")\n",
    "\n",
    "    mrg = (model_level12.valid_features.groupby([\"ds\"])[\"y\"].sum().reset_index()\n",
    "           .merge(forecast_v0.groupby([\"ds\"])[\"y_pred\"].sum().reset_index(), on=\"ds\")\n",
    "           .merge(forecast_v1.groupby([\"ds\"])[\"y_pred\"].sum().reset_index(), on=\"ds\"))\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot_date(mrg.ds, mrg.y, \"o-\", label=\"real\")\n",
    "    plt.plot_date(mrg.ds, mrg.y_pred_x, \"o-\", label=\"pred_v0\")\n",
    "    plt.plot_date(mrg.ds, mrg.y_pred_y, \"o-\", label=\"pred_v1\")\n",
    "    plt.grid()\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "    forecast_v1[\"store_id\"] = store_id\n",
    "    stores_forecast.append(forecast_v1)\n",
    "    del model_level12, _train_data, _valid_index, evaluator\n",
    "    gc.collect()\n",
    "    \n",
    "fold_forecast = pd.concat(stores_forecast, ignore_index=True)\n",
    "mrg = pd.merge(data.loc[:, [\"ds\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\",\"y\"]],\n",
    "               fold_forecast, how=\"inner\", on=[\"ds\",\"item_id\",\"store_id\"])\n",
    "evaluator = WRMSSEEvaluator(mrg.loc[:, [\"ds\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\",\"y\"]])\n",
    "wrmsse = evaluator._evaluate(mrg.y_pred.values)\n",
    "print(\"\\nwrmsse:\", wrmsse)\n",
    "print(evaluator.errors_by_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
